\documentclass{extarticle}
\sloppy
\input{packages.tex}
\input{math_commands.tex}

\title{\vspace{-2em}Chapter 9: Multilinear Algebra and Determinants}
\author{\emph{Linear Algebra Done Right (4th Edition)}, by Sheldon Axler}
\date{Last updated: \today}

\begin{document}
\maketitle 
\tableofcontents
\newpage 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9A %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{9A: Bilinear Forms and Quadratic Forms}
\addcontentsline{toc}{section}{9A: Bilinear Forms and Quadratic Forms}

\begin{definition}[bilinear form]
    A \textbf{bilinear form} on \(V\) is a function \(\beta \colon V \times V \to \F\) 
    such that 
    \[v \mapsto \beta(v, u) \text{  and  } v \mapsto \beta(u,v)\]
    are both linear functionals on \(V\) for every \(u \in V\).
\end{definition}

\begin{remark}
    A better but less popular terminology is ``bilinear functional''. If \(V\) is real, 
    then the function \((u, v) \mapsto \langle u,v \rangle\) is a bilinear form. If \(V\) is 
    complex, then it isn't.
\end{remark}

\begin{remark}
    If \(\F = \R\), then a bilinear form differs from an inner product in that it does not require 
    positive definiteness or symmetry. 
\end{remark}

\begin{remark}
    A bilinear form \(\beta\) on \(V\) is a linear map on \(V \times V\) only if \(\beta = 0\).
\end{remark}

\begin{definition}[\(V^{(2)}\)]
    The set of bilinear forms on \(V\) is denoted by \(V^{(2)}\).
\end{definition}

\begin{definition}[matrix of a bilinear form, \(\Mc(\beta)\)]
    Suppose \(\beta\) is a bilinear form on \(V\) and \(e_1, \ldots, e_n\) is a basis of \(V\). The \textbf{matrix}
    of \(\beta\) with respect to this basis is the \(n\)-by-\(n\) matrix matrix \(\Mc(\beta)\) whose entry 
    \(\Mc(\beta)_{j, k}\) in row \(j\), column \(k\) is given by 
    \[\Mc (\beta)_{j, k} = \beta (e_j, e_k)\]
    If the basis \(e_1, \ldots, e_n\) is not clear from the context, then the notation 
    \(\Mc \left( \beta, (e_1, \ldots, e_n) \right)\) is used. 
\end{definition}

\begin{corollary}[\(\dim V^{(2)} = \left( \dim V \right)^2\)]
    Suppose \(e_1, \ldots, e_n\) is a basis of \(V\). Then the map \(\beta \mapsto \Mc(\beta)\) is an 
    isomorphism of \(V^{(2)}\) onto \(\F^{n, n}\). Furthermore, \(\dim V^{(2)} = (\dim V)^2\). 
\end{corollary}

\begin{lemma}[composition of a bilinear form and an operator]
    Suppose \(\beta\) is a bilinear form on \(V\) and \(T \in \Lc(V)\). Define bilinear forms 
    \(\alpha\) and \(\rho\) on \(V\) by 
    \[\alpha(u, v) = \beta(u, Tv) \text{  and  } \rho(u,v) = \beta(Tu, v)\]
    Let \(e_1, \ldots, e_n\) be a basis of \(V\). Then 
    \[\Mc(\alpha) = \Mc(\beta) \Mc(T) \text{  and  } \Mc(\rho) = \Mc(T)^\top \Mc(\beta)\]
\end{lemma}

\begin{thm}[change-of-basis formula]
    Suppose \(\beta \in V^{(2)}\). Suppose \(e_1, \ldots, e_n\) and \(f_1, \ldots, f_n\) are bases of \(V\). 
    Let 
    \[A = \Mc(\beta (e_1, \ldots, e_n)) \text{   and   } B = \Mc \left( \beta , \left( f_1, \ldots, f_n \right) \right)\]
    and \(C = \Mc \left( I, \left( e_1, \ldots, e_n \right), \left( f_1, \ldots, f_n \right) \right)\). Then 
    \[A = C^\top B C\]
\end{thm}

\begin{definition}[symmetric bilinear form, \(V_{sym}^{(2)}\)]
    A bilinear form \(\rho \in V^{(2)}\) is called \textbf{symmetric} if 
    \[\rho (u, w) = \rho (w, u)\]
    for all \(u, w \in V\). The set of symmetric bilinear forms on \(V\) is denoted by \(V_{sym}^{(2)}\).
\end{definition}

\begin{remark}
    For real inner product space, define \(\rho (u,w) = \langle u,w \rangle \in V_{sym}^{(2)}\). Additional 
    example include 
    \[\rho (u,w) = \langle u,Tw \rangle\]
    where \(T\) is self-adjoint and  
    \[\rho (S,T) = \tr \ (ST)\]
    where here \(\rho \colon \Lc(V) \times \Lc(V) \to \F\).
\end{remark}

\begin{definition}[symmetric matrix]
    A square matrix \(A\) is called \textbf{symmetric} if it equals its transpose. 
\end{definition}

\begin{thm}[symmetric bilinear forms are diagonalizable]
    Suppose \(\rho \in V^{(2)}\). Then the following are equivalent. 

    \begin{enumerate}[label=(\alph*)]
        \item \(\rho\) is a symmetric bilinear form on \(V\). 
        \item \(\Mc(\rho, (e_1, \ldots, e_n))\) is a symmetric matrix for every basis \(e_1, \ldots, e_n\) of \(V\). 
        \item \(\Mc(\rho, (e_1, \ldots, e_n))\) is a symmetric matrix for some basis \(e_1, \ldots, e_n\) of \(V\). 
        \item \(\Mc(\rho, (e_1, \ldots, e_n))\) is a diagonal matrix for some basis \(e_1, \ldots, e_n\) of \(V\).
    \end{enumerate}
\end{thm}

\begin{thm}
    Suppose \(V\) is a real inner product space and \(\rho\) is a symmetric bilinear form on \(V\). Then 
    \(\rho\) has a diagonal matrix with respect to some orthonormal basis of \(V\).
\end{thm}

\begin{definition}[alternating bilinear form, \(V_{alt}^{(2)}\)]
    A bilinear form \(\alpha \in V^{(2)}\) is called \textbf{alternating} if 
    \[\alpha(v, v) = 0\]
    for all \(v \in V\). The set of alternating bilinear forms on \(V\) is denoted by \(V_{alt}^{(2)}\).
\end{definition}

\begin{lemma}[characterization of alternating linear forms]
    A bilinear form \(\alpha\) on \(V\) is alternating if and only if 
    \[\alpha(u, w) = -\alpha(w, u)\]
    for all \(u, w \in V\).
\end{lemma}

\begin{thm}
    The sets \(V_{sym}^{(2)}\) and \(V_{alt}^{(2)}\) are subspaces of \(V^{(2)}\). Furthermore, 
    \[V^{(2)} = V_{sym}^{(2)} \oplus V_{alt}^{(2)}\]
\end{thm}

\begin{definition}[quadratic form associated with a bilinear form, \(q_\beta\)]
    For \(\beta\) a bilinear form on \(V\), define a function \(q_\beta \colon V \to \F\) by 
    \(q_\beta (v) =\beta(v, v)\). A function \(q \colon V \to \F\) is called a \textbf{quadratic form} 
    on \(V\) if there exists a bilinear form \(\beta\) on \(V\) such that \(q = q_\beta\).
\end{definition}

\begin{corollary}[quadratic form on \(\F^n\)]
    Suppose \(n\) is a positive integer and \(q\) is a function from \(\F^n\) to \(\F\). Then \(q\) is a 
    quadratic form on \(\F^n\) if and only if there exist numbers \(A_{j, k} \in \F\) for \(j, k \in \{1, \ldots, n\}\)
    such that 
    \[q(x_1, \ldots, x_n)  = \sum_{k=1}^{n} \sum_{j=1}^{n} A_{j, k} x_j x_k\]
    for all \((x_1, \ldots, x_n) \in \F^n\). 
\end{corollary}

\begin{thm}[characterizations of quadratic forms]
    Suppose \(q \colon V \to \F\) is a function. Then following are equivalent. 
    \begin{enumerate}[label=(\alph*)]
        \item \(q\) is a quadratic form. 
        \item There exists a unique symmetric bilinear form \(\rho\) on \(V\) such that \(q = q_\rho\). 
        \item \(q(\lambda v) = \lambda^2 q(v)\) for all \(\lambda \in \F\) and all \(v \in V\), and the function 
        \[(u, w) \mapsto q(u + w) - q(u) - q(w)\]
        is a symmetric bilinear form on \(V\). 

        \item \(q(2v) = 4 q(v)\) for all \(v \in V\), and the function 
        \[(u, w) \mapsto q(u + w) - q(u) - q(w)\]
        is a symmetric bilinear form on \(V\). 
    \end{enumerate}
\end{thm}

\begin{thm}[diagonalization of quadratic form]
    Suppose \(q\) is a quadratic form on \(V\). 
    \begin{enumerate}[label=(\alph*)]
        \item There exist a basis \(e_1, \ldots, e_n\) of \(V\) and \(\lambda_1, \ldots, \lambda_n \in \F\) 
        such that 
        \[q(x_1 e_1 + \cdots + x_n e_n) = \lambda_1 x_1^2 + \cdots + \lambda_n x_n^2\]
        for all \(x_1, \ldots, x_n \in \F\). 

        \item If \(\F = \R\) and \(V\) is an inner product space, then the basis in (a) can be chosen to be 
        an orthonormal basis of \(V\). 
    \end{enumerate}
\end{thm}

\begin{remark}
    For each quadratic form we can choose a basis such that the quadratic form looks like a weighted sum of 
    squares of the coordinates. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9A PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\addcontentsline{toc}{subsection}{9A Problem Sets}
\begin{problem}{1}
    Prove that if \(\beta\) is a bilinear form on \(\F\), then there exists \(c \in \F\) such that 
    \[\beta (x, y) = c xy\]
    for all \(x, y \in \F\).
\end{problem}

\begin{proof}
We note that since the input is taken from \(\F\), the basis is naturally \(1\). So we have that 
\[\beta(x, y) = x \beta(1, y) = xy \beta(1,1) = c xy\]

where we take \(c = \beta(1,1)\). 
\end{proof}

\begin{problem}{2}
    Let \(n = \dim V\). Suppose \(\beta\) is a bilinear form on \(V\). Prove that there exist 
    \(\phi_1, \ldots, \phi_n, \tau_1, \ldots, \tau_n \in V'\) such that 
    \[\beta(u, v) = \phi_1(u) \cdot \tau_1(v) + \cdots + \phi_n (u) \cdot \tau_n (v)\]
    for all \(u, v \in V\). 
\end{problem}

\begin{proof}
\begin{align*}
    \beta(u, v) 
    &= \beta \left(\sum_{i=1}^{n} u_i e_i, \sum_{i=1}^{n} v_j e_j\right) 
    = \sum_{i=1}^{n} \sum_{j=1}^{n} u_i v_j \beta(e_i, e_j)
\end{align*}

We can now define the linear function \(\phi_i (u) = u_i = e_i^*(u)\) and 
\(\tau'_j(v) = v_j = e_j^*(v)\). Then we have that 

\[\beta(u, v) = \sum_{i=1}^{n} \phi_i(u) \left( \sum_{j=1}^{n} \beta(e_i, e_j) \tau_j'(v) \right) 
= \sum_{i=1}^{n} \phi(u) \tau_i(v)\]
\end{proof}

\begin{problem}{3}
    Suppose \(\beta \colon V \times V \to \F\) a bilinear form on \(V\) and also is a linear functional 
    on \(V \times V\). Prove that \(\beta = 0\).
\end{problem}

\begin{proof}
First we show that \(\beta \in V_{alt}^{(2)}\). Take any \(u \in V\), then we have 
\begin{align*}
    \beta((u, u) + (u, u)) &= 2 \beta(u, u) \\ 
    \beta(2u, 2u) &= 4 \beta(u, u)
\end{align*}

this shows that \(\beta(u, u) = 0\) for all \(u\). Next, we show the off-diagonal terms are 0: first, 

\begin{align*}
    \beta(u, w) = \sum_{i=1}^{n} \sum_{j=1}^{n} u_i w_j \beta(e_i, e_j) \tag*{bilinearity}
\end{align*}

at the same time, 
\begin{align*}
    \beta(u, w) &= \beta \left( \sum_{i=1}^{n}u_i e_i, \sum_{j=1}^{n} w_j e_j \right) \\ 
    &= \beta \left( \sum_{i=1}^{n} \left( u_i e_i, w_i e_i \right) \right) \\ 
    &= \sum_{i=1}^{n} \beta \left( u_i e_i, w_i e_i \right) \tag*{linearity on \(V \times V\)}  \\ 
    &= \sum_{i=1}^{n} u_i w_i \beta(e_i, e_i)
\end{align*}

This shows that all off-diagonal terms are 0, i.e., \(\beta(e_i, e_j) = 0\) for all \(i \neq 0\). Therefore, 
we have \(\beta = 0\).
\end{proof}

\begin{problem}{6}
    Prove or give a counterexample: If \(\rho\) is a symmetric bilinear form on \(V\), then 
    \[\{v \in V \colon \rho (v, v) = 0\}\]
    is a subspace of \(V\). 
\end{problem}

\begin{proof}
Consider \(V = \R^2\) and \(\rho(x, y) = x_1 y_1 - x_2 y_2\). Let \(x = (1, 1), y = (-1, 1 )\), then we 
have that \(\rho(x, x) = 1 - 1 = 0, \rho(y, y) = 1 - 1 = 0\), but \(\rho(x+y, x+y) = 0 - 4 = -4 \neq 0\).
\end{proof}

\begin{problem}{8}
    Find formulas for \(\dim V_{sym}^{(2)}\) and \(\dim V_{alt}^{(2)}\) in terms of \(\dim V\). 
\end{problem}

\begin{proof}
Let \(\dim V = n\). For \(\beta \in V_{sym}^{(2)}\), consider \(\Mc(\beta)\). Its diagonal entries can be 
chosen arbitrarily. For off-diagonal entries, only half of them can be chosen arbitrarily, therefore the dimension 
is 
\[\frac{n(n-1)}{2} + n = \frac{n(n+1)}{2}\]

For \(\beta \in V_{alt}^{(2)}\), consider \(\Mc(\beta)\). The diagonal entires are all 0 and only half of the off-diagonal entries 
can be chosen arbitrarily. Therefore, the dimension is 
\(\frac{n(n-1)}{2}\).
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9B %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\section*{9B: Alternating Multilinear Forms}
\addcontentsline{toc}{section}{9B: Alternating Multilinear Forms}

\begin{definition}[\(V^m\)]
    For \(m\) a positive integer, define \(V^m\) by 
    \[V^m = \underbrace{V \times \cdots \times V}_{m \text{ times}}\]
\end{definition}

\begin{definition}[m-linear form, \(V^{(m)}\), multilinear form]
    Below we introduce the definitions.
    \begin{itemize}
        \item For \(m\) a positive integer, an \textbf{m-linear form} on \(V\) is a function 
        \(\beta \colon V^m \to \F\) that is linear in each slot when the other slots are held fixed. This means 
        that for each \(k \in \{1, \ldots, m\}\) and all \(u_1, \ldots, u_m \in V\), the function 
        \[v \mapsto \beta(u_1, \ldots, u_{k-1}, v, u_{k+1}, \ldots, u_m)\]
        is a linear map from \(V\) to \(\F\).
        
        \item The set of \(m\)-linear forms on \(V\) is denoted by \(V^{(m)}\). 
        \item A function \(\beta\) is called a \textbf{multilinear form} on \(V\) if it is an \(m\)-linear form on 
        \(V\) for some positive integer \(m\).
    \end{itemize}
\end{definition}

\begin{remark}
    A 1-linear form on \(V\) is a linear functional on \(V\). A 2-linear form on \(V\) is a bilinear form 
    on \(V\). \(V^{(m)}\) is a vector space. 
\end{remark}

\begin{example}[m-linear forms]
    Suppose \(\alpha, \beta \in V^{(2)}\). Define a function \(\beta \colon V^4 \to \F\) by 
    \[\beta(v_1, v_2, v_3, v_4) = \alpha(v_1, v_2) \beta(v_3, v_4)\] 
    Then \(\beta \in V^{(4)}\).
\end{example}

\begin{example}[m-linear forms]
    Define \(\beta \colon \left( \Lc \left( V \right) \right)^m \to \F\) by 
    \[\beta(T_1, \ldots, T_m) = \tr \ (T_1 \cdots T_m)\]
    Then \(\beta\) is an \(m\)-linear form on \(\Lc(V)\).
\end{example}

\begin{definition}[alternating forms, \(V_{alt}^{(m)}\)]
    Suppose \(m\) is a positive integer. 
    \begin{itemize}
        \item An \(m\)-linear form \(\alpha\) on \(V\) is called \textbf{alternating} if \(\alpha(v_1, \ldots, v_m) = 0\) 
        whenever \(v_1, \ldots, v_m\) is a list of vectors in \(V\) with \(v_j = v_k\) for some two distinct values 
        of \(j\) and \(k\) in \(\{1, \ldots, m\}\).
        
        \item \(V_{alt}^{(m)} = \{\alpha \in V^{(m)} \colon \alpha \text{ is an alternating m-linear form on V}\}\).
    \end{itemize}
\end{definition}

\begin{corollary}
    Suppose \(m\) is a positive integer and \(\alpha\) is an alternating \(m\)-linear form on \(V\). If 
    \(v_1, \ldots, v_m\) is a linearly dependent list in \(V\), then 
    \[\alpha(v_1, \ldots, v_m) = 0\]
\end{corollary}

\begin{corollary}
    Suppose \(m  > \dim V\). Then 0 is the only alternating \(m\)-linear form on \(V\).
\end{corollary}

\begin{thm}[swapping input vectors in an alternating multilinear form]
    Suppose \(m\) is a positive integer, \(\alpha\) is an alternating \(m\)-linear form on \(V\), and 
    \(v_1, \ldots, v_m\) is a list of vectors in \(V\). Then swapping the vectors in any two slots 
    of \(\alpha(v_1, \ldots, v_m)\) changes the value of \(\alpha\) by a factor of \(-1\).
\end{thm}

\begin{remark}
    An odd numer of swaps cause the value of \(\alpha\) to change by a factor of -1 and it won't change with 
    an even number of swaps. 
\end{remark}

\begin{definition}[permutation, perm \(m\)]
    Suppose \(m\) is a positive integer. 
    \begin{itemize}
        \item A \textbf{permutation} of \((1, \ldots, m)\) is a list \((j_1, \ldots, j_m)\) that contains 
        each of the number \(1, \ldots, m\) exactly once. 
        \item The set of permutations of \((1, \ldots, m)\) is denoted by perm \(m\). 
    \end{itemize}
\end{definition}

\begin{definition}[sign of a permutation]
    The \textbf{sign} of a permutation \((j_1, \ldots, j_m)\) is defined by 
    \[\text{sign}(j_1, \ldots, j_m) = (-1)^N\]
    where \(N\) is the number of pairs of integers \((k, l)\) with \(1 \leq k < l \leq m\) such that 
    \(k\) appears after \(l\) in the list \((j_1, \ldots, j_m)\).
\end{definition}

\begin{lemma}
    Swapping two entries in a permutation multiplies the sign of the permutation by -1.
\end{lemma}

\begin{lemma}[permutation and alternating multilinear form]
    Suppose \(m\) is a positive integer and \(\alpha \in V_{alt}^{(m)}\). Then 
    \[\alpha(v_{j_1}, \ldots, v_{j_m}) = \left( \text{sign} \left( j_1, \ldots, j_m \right) \right) 
    \alpha \left( v_1, \ldots, v_m \right)\]
    for every list \(v_1, \ldots, v_m\) of vectors in \(V\) and all \((j_1, \ldots, j_m) \in \text{perm } m\).
\end{lemma}

\begin{thm}
    Let \(n = \dim V\). Suppose \(e_1, \ldots, e_n\) is a basis of \(V\) and \(v_1, \ldots, v_n \in V\). For 
    each \(k \in \{1, \ldots, n\}\), let \(b_{1, k}, \ldots, b_{n, k} \in \F\) be such that 
    \[v_k = \sum_{j=1}^{n} b_{j, k} e_j\]
    Then 
    \[\alpha(v_1, \ldots, v_n) = \alpha(e_1, \ldots, e_n) \sum_{(j_1, \ldots, j_n) \in \text{perm } n} 
    \left( \text{sign} \left( j_1, \ldots, j_n \right) \right) b_{j_1 , 1} \cdots b_{j_n, n}\]
    for every alternating n-linear form \(\alpha\) on \(V\).
\end{thm}

\begin{thm}
    The vector space \(V_{alt}^{(dim V)}\) has dimension one.
\end{thm}

\begin{corollary}
    Let \(n = \dim V\),. Suppose \(\alpha\) is a nonzero alternating n-linear form on \(V\) and 
    \(e_1, \ldots, e_n\) is a list of vectors in \(V\). Then 
    \[\alpha (e_1, \ldots, e_n) \neq 0\]
    if and only if \(e_1, \ldots, e_n\) is linearly independent. 
\end{corollary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9B PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\addcontentsline{toc}{subsection}{9B Problem Sets}

\begin{problem}{1}
    Suppose \(m\) is a positive integer. Show that \(\dim V^{(m)} = \left( \dim V \right)^m\).
\end{problem}

\begin{proof}
Let \(\dim V = n\) with basis \(e_1, \ldots, e_n\). The basis vector for \(V^{(m)}\) can be formed 
via taking all possible \(m\)-tuples \(b_{j_1}, \ldots, b_{j_m}\) where \(b_{j_i}\) is a component of the basis. 
There are \(n\) choices over \(m\) positions, so we have that \(\dim V^{(m)} = \left( \dim V \right)^m\).
\end{proof}

\begin{problem}{3}
    Suppose \(m\) is a positive integer and \(\alpha\) is an \(m\)-linear form on \(V\) such that 
    \(\alpha(v_1, \ldots, v_m) = 0\) whenver \(v_1, \ldots, v_m\) is a list of vectors in \(V\) with 
    \(v_j = v_{j+1}\) for some \(j \in \{1, \ldots, m-1\}\) Prove that \(\alpha\) is an alternating 
    \(m\)-linear form on \(V\).
\end{problem}

\begin{proof}
Note that if the list \(v_1, \ldots, v_n\) comes with consecutive identical numbers, then by definition the 
output becomes 0. To prove \(\alpha\) to be an alternating \(m\)-linear form, considers \(v_i = v_k\) for 
\(i+1 < k\). Note that then we can now just swap and gets the same result: 
\begin{align*}
    \alpha(v_1, \ldots, v_i, v_{i+1}, \ldots, v_k, \ldots, v_n)  
    = - \alpha(v_1, \ldots, v_i, v_{k}, \ldots, v_{i+1}, \ldots, v_n) = 0  
\end{align*}
\end{proof}

\begin{problem}{5}
    Suppose \(m\) is a positive integer and \(\beta\) is an \(m\)-linear form on \(V\). Define an 
    \(m\)-linear form \(\alpha\) by 
    \[\alpha(v_1, \ldots, v_m) = \sum_{(j_1, \ldots, j_m) \in \text{perm } m} 
    \left( \text{sign} \left( j_1, \ldots, j_m \right) \beta(v_{j_1}, \ldots, v_{j_m}) \right)\]

    for \(v_1, \ldots, v_m \in V\). Explain why \(\alpha \in V_{alt}^{(m)}\).
\end{problem}

\begin{proof}
If there are two repeating vectors, let's say \(v_p = v_q\), then we know that 
\[\beta(v_1, \ldots, v_p, \ldots, v_q, \ldots, v_m) = \beta(v_1, \ldots, v_q, \ldots, v_p, \ldots, v_m)\]

However, through swapping, the coefficient differs by (-1), so we have 
\begin{align*}
    &\text{sign}(1, \ldots, p, \ldots, q, \ldots, m) \beta(v_1, \ldots, v_p, \ldots, v_q, \ldots, v_m) \\
= - &\text{sign}(1, \ldots, q, \ldots, p, \ldots, m) \beta(v_1, \ldots, v_q, \ldots, v_p, \ldots, v_m)
\end{align*}

% \[\text{1, \ldots, p, \ldots, q, \ldots, m} \beta(v_1, \ldots, v_p, \ldots, v_q, \ldots, v_m) 
% = - \text{1, \ldots, q, \ldots, p, \ldots, m} \beta(v_1, \ldots, v_q, \ldots, v_p, \ldots, v_m)\]

This basically shows the main idea of the proof. To make this more rigorous, we claim that 
for each permutation \(\sigma \in\) perm \(m\), there is a corresponding permutation 
\(\sigma_{pq} \in\) perm \(m\) such that keeps everything unchanged while only swapping the position of 
\(p\) and \(q\). This means that for each permutation, there is a corresponding ``cancelling'' pair 
permutation. Since we are summing all permutations, the result is finally 0, finishing the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9C %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\section*{9C: Determinants}
\addcontentsline{toc}{section}{9C: Determinants}

\begin{definition}[\(\alpha_T\)]
    Suppose that \(m\) is a positive integer and \(T \in \Lc(V)\). For \(\alpha \in V_{alt}^{(m)}\), define 
    \(\alpha_T \in V_{alt}^{(m)}\) by 
    \[\alpha_T(v_1, \ldots, v_m) = \alpha(Tv_1, \ldots, Tv_m)\]
    for each list \(v_1, \ldots, v_m\) of vectors in \(V\).
\end{definition}

\begin{remark}
    The function \(\alpha \mapsto \alpha_T\) is a linear map of \(V_{alt}^{(m)}\) to itself. We know 
    that \(\dim V_{alt}^{(\dim V)} = 1\), so the linear map is simply a multiplication by some unique scalar.
    For the linear map \(\alpha \mapsto \alpha_T\), we now define \(\det T\) to be that scalar.
\end{remark}

\begin{definition}[\textbf{determinant} of an operator, \(\det T\)]
    Suppose \(T \in \Lc(V)\). The \textbf{determinant} of \(T\), denoted by \(\det T\), is defined to be the 
    unique number in \(\F\) such that 
    \[\alpha_T = (\det T) \alpha\]
    for all \(\alpha \in V_{alt}^{(\dim V)}\).
\end{definition}

\begin{remark}
    Let \(n = \dim V\). 

    \begin{itemize}
        \item If \(I\) is the identity operator on \(V\), then \(\alpha_I = \alpha\) for all \(\alpha \in V_{alt}^{(n)}\). 
        This gives that \(\det I = 1\).
        
        \item More generally, if \(\lambda \in \F\), then \(\alpha_{\lambda I} = \lambda^n \alpha\) 
        for all \(\alpha \in V_{alt}^{(n)}\). Thus \(\det (\lambda I) = \lambda^n\).

        \item Since \(\alpha_{\lambda T} = \lambda^n \alpha_T = \lambda^n (\det T) \alpha\) for all  
        \(\alpha \in V_{alt}^{(n)}\), \(\det (\lambda T) = \lambda^n \det T\). 

        \item Suppose \(T \in \Lc(V)\) and there is a basis \(e_1, \ldots, e_n\) of \(V\) consisting of 
        eigenvectors of \(T\), with corresponding eigenvalues \(\lambda_1, \ldots, \lambda_n\). If 
        \(\alpha \in V_{alt}^{(n)}\), then 
        \[\alpha_T (e_1, \ldots, e_n) = \alpha (\lambda_1 e_1, \ldots, \lambda_n e_n) = (\lambda_1 \cdots \lambda_n) \alpha(e_1, \ldots, e_n)\]

        If \(\alpha \neq 0\), then \(\alpha(e_1, \ldots, e_n) \neq 0\). Thus this means that 
        \[\det T = \lambda_1 \cdots \lambda_n\]
    \end{itemize}
\end{remark}

\begin{definition}[determinant of a matrix, det \(A\)]
    Suppose that \(n\) is a positive integer and \(A\) is an \(n\)-by-\(n\) matrix square matrix with entries in \(\F\). 
    Let \(T \in \Lc(\F^n)\) be the operator whose matrix with respect to the standard basis of \(\F^n\) 
    equals \(A\). The \textbf{determinant} of \(A\), denoted by \(\det A\), is defined by 
    \(\det A = \det T\).
\end{definition}

\begin{thm}[determinant is an alternating multilinear form]
    Suppose that \(n\) is a positive integer. The map that takes a list \(v_1, \ldots, v_n\) of vectors 
    in \(\F^n\) to \(\det (v_1 \cdots v_n)\) is an alternating \(n\)-linear form on \(\F^n\).
\end{thm}

\begin{corollary}[formula for determinants of a matrix]
    Suppose that \(n\) is a positive integer and \(A\) is an \(n\)-by-\(n\) matrix square matrix. Then 
    \[\det A = \sum_{(j_1, \ldots, j_n) \in \text{perm } n} \left( \text{sign}(j_1, \ldots, j_n) \right) 
    A_{j_1, 1} \cdots A_{j_n, n}\]
\end{corollary}

\begin{remark}
    The sum in the formula above contains \(n!\) terms. 
\end{remark}

\begin{corollary}[determinant of upper-triangular matrix]
    Suppose that \(A\) is an upper-triangular matrix with \(\lambda_1, \ldots, \lambda_n\) on the diagonal. 
    Then \(\det A = \lambda_1 \cdots \lambda_n\). 
\end{corollary}

\begin{thm}[determinant is multiplicative]
    We have the following result: 
    \begin{enumerate}[label=(\alph*)]
        \item Suppose \(S, T \in \Lc(V)\). Then \(\det (ST) = \det (S) \det (T)\). 
        \item Suppose \(A\) and \(B\) are square matrices of the same size. Then 
        \[\det (AB) = \det (A) \det (B)\]
    \end{enumerate}
\end{thm}

\begin{corollary}
    An operator \(T \in \Lc(V)\) is \textbf{invertible} if and only if \(\det T \neq 0\). Furthermore, 
    if \(T\) is invertible, then \(\det (T^{-1}) = \frac{1}{\det T}\).
\end{corollary}

\begin{corollary}
    Suppose \(T \in \Lc(V)\) and \(\lambda \in \F\). Then \(\lambda\) is an eigenvalue of \(T\) if and 
    only if \(\det (\lambda I - T) = 0\).
\end{corollary}

\begin{corollary}
    Suppose \(T \in \Lc(V)\) and \(S \colon W \to V\) is an invertible linear map. Then 
    \[\det (S^{-1} T S) = \det T\]
\end{corollary}

\begin{corollary}
    Suppose \(T \in \Lc(V)\) and \(e_1, \ldots, e_n\) is a basis of \(V\). Then 
    \[\det T = \det \Mc \left( T, (e_1, \ldots, e_n) \right)\]
\end{corollary}

\begin{corollary}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Then \(\det T\) equals the product of the eigenvalues of 
    \(T\), with each eigenvalue included as many times as its multiplicity. 
\end{corollary}

\begin{corollary}[determinant of transpose, dual, or adjoint]
    We have the following result: 
    \begin{enumerate}[label=(\alph*)]
        \item Suppose \(A\) is a square matrix. Then \(\det A^\top = \det A\). 
        \item Suppose \(T \in \Lc(V)\). Then \(\det T' = \det T\). 
        \item Suppose \(V\) is an inner product space and \(T \in \Lc(V)\). Then 
        \[\det (T^*) = \overline{\det T}\]
    \end{enumerate}
\end{corollary}

\begin{corollary}
    Helpful results in evaluating the determminants: 

    \begin{enumerate}[label=(\alph*)]
        \item If either two columns or two rows of a square matrix are equal, then the determinant of the 
        matrix equals 0. 
        \item Suppose \(A\) is a square matrix and \(B\) is the matrix obtained from \(A\) by swapping either 
        two columns or two rows. Then \(\det A = - \det B\).
        \item If one column or one row of a square matrix is multiplied by a scalar, then the value of the 
        determinant is multiplied by the same scalar. 
        \item If a scalar multiple of one column of a square matrix to added to another column, then the value 
        of the determinant is unchanged. 
        \item If a scalar multiple of one row of a square matrix to added to another row, then the value of 
        the determinant is unchanged. 
    \end{enumerate}
\end{corollary}

\begin{corollary}
    Suppose \(V\) is an inner product space and \(S \in \Lc(V)\) an unitary operator. Then 
    \(|\det S| = 1\).
\end{corollary}

\begin{corollary}
    Suppose \(V\) is an inner product space and \(T \in \Lc(V)\) is a positive operator. Then 
    \(\det T \geq 0\).
\end{corollary}

\begin{corollary}
    Suppose \(V\) is an inner product space and \(T \in \Lc(V)\). Then 
    \[|\det T| = \sqrt{\det (T^* T)} = \text{ product of singular values of } T\]
\end{corollary}

\begin{lemma}
    Suppose \(\F = \C\) and \(T \in \Lc(V)\). Let \(\lambda_1, \ldots, \lambda_m\) denote the distinct 
    eigenvalues of \(T\), and let \(d_1, \ldots, d_m\) denote their multiplicities. Then 
    \[\det (zI - T) = (z - \lambda_1)^{d_1} \cdots (z - \lambda_m)^{d_m}\]
\end{lemma}

\begin{definition}[characteristic polynomial]
    Suppose \(T \in \Lc(V)\). The polynomial defined by 
    \[z \mapsto \det (zI - T)\]
    is called the \textbf{characteristic polynomial} of \(T\).
\end{definition}

\begin{thm}[Cayley-Hamilton theorem]
    Suppose \(T \in \Lc(V)\) and \(q\) is the characteristic polynomial of \(T\). Then \(q(T) = 0\).
\end{thm}

\begin{corollary}[characteristic polynomial, trace, and determinant]
    Suppose \(T \in \Lc(V)\). Let \(n = \dim V\). Then the characteristic polynomial of \(T\) can be written 
    as 
    \[z^n - (\tr \ T) z^{n-1} + \cdots  + (-1)^n (\det T)\]
\end{corollary}

\begin{thm}[Hadamard's inequality]
    Suppose \(A\) is an \(n\)-by-\(n\) matrix matrix. Let \(v_1, \ldots, v_n\) denote the columns of \(A\). Then 
    \[|\det A| \leq \prod_{k=1}^{n} \norm{v_k}\]
\end{thm}

\begin{thm}[determinant of Vandermonde matrix]
    Suppose \(n > 1\) and \(\beta_1, \ldots, \beta_n \in \F\). Then 

    \begin{align*}
        \text{det} \begin{pmatrix}
            1 & \beta_1 & \beta_1^2 & \cdots & \beta_1^{n-1} \\
            1 & \beta_2 & \beta_2^2 & \cdots & \beta_2^{n-1} \\
            1 & \beta_3 & \beta_3^2 & \cdots & \beta_3^{n-1} \\
             &  &  & \ddots &  \\
            1 & \beta_n & \beta_n^2 & \cdots & \beta_n^{n-1}
            \end{pmatrix} = \prod_{1 \leq j < k \leq n} (\beta_k - \beta_j).
    \end{align*}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9C PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\addcontentsline{toc}{subsection}{9C Problem Sets}

\begin{problem}{1}
    Prove or give a counterexample: \(S, T \in \Lc(V) \Rightarrow \det (S+T) = \det S + \det T\). 
\end{problem}

\begin{proof}
Consider \(\R^2\), and that 
\[\Mc(S) = \begin{bmatrix}
    1 & 0 \\ 
    0 & 1
\end{bmatrix} \ \ \ \Mc(T) = \begin{bmatrix}
    1 & 0 \\ 
    0 & 2
\end{bmatrix}\]
Then clearly \(\det S = 1\) and \(\det T = 2\). However, we have that  
\[\Mc(S) + \Mc(T) = \begin{bmatrix}
    2 & 0 \\ 
    0 & 2
\end{bmatrix}\]
which has \(\det (S+T) = 6 \neq \det S + \det T\).
\end{proof}

\begin{problem}{3}
    Suppose \(T \in \Lc(V)\) is nilpotent. Prove that \(\det (I + T) = 1\).
\end{problem}

\begin{proof}
We know that 0 is the only eigenvalue of \(T\) and thus the only eigenvalue of \(I + T\) is 1. Hence 
\(\det (I+T) = 1\).
\end{proof}

\begin{problem}{5}
    Suppose \(A\) is a block triangular matrix 
    \[A = \begin{pmatrix}
        A_1 & & * \\ 
           & \ddots & \\ 
        0 & & A_m
    \end{pmatrix}\]
    where each \(A_k\) along the diagonal is a square matrix. Prove that 
    \[\det A = (\det A_1) \cdots (\det A_m)\]
\end{problem}

\begin{proof}
One can show that \(\det A = (\det A_1) (\det A_2)\) through direct proof. We use induction on \(m\) for 
solving this problem. The base case is trivial. We assume the statement holds for \(m \leq k-1\). Then 
for \(m = k\), we can partition the matrix into two blocks: 
\[\begin{bmatrix}
    A' & * \\ 
    0 & A_k 
\end{bmatrix}\]

where 
\[A' = \begin{bmatrix}
    A_1 & & * \\ 
    & \ddots & \\ 
    0 & & A_{k-1}
\end{bmatrix}\]

Then we have that \(\det A = (\det A') (\det A_k) = (\det A_1) \cdots (\det A_k)\), finishing the proof. 
\end{proof}

\begin{problem}{9}
    Suppose that \(V\) is a real vector space of even dimension, \(T \in \Lc(V)\), and \(\det T < 0\). 
    Prove that \(T\) has at least two distinct eigenvalues. 
\end{problem}

\begin{proof}
Since \(\det (T) \neq 0\), \(T\) is invertible and thus have \(n\) distinct eigenvalues with 
\(n \geq 2\). Another argument could be that for real cases, there have to be at least one negative and 
one positive eigenvalue to make the determinant negative; for complex cases, there must be 
two conjugate pairs. 
\end{proof}

\begin{problem}{11}
    Prove or give a counter example: If \(\F = \R, T \in \Lc(V)\), and \(\det T > 0\), then 
    \(T\) has a square root. 
\end{problem}

\begin{proof}
Not necessarily. Consider an operator in \(\R^2\) with two negative eigenvalues which is clearly 
non-positive and therefore does not have a square root. 
\end{proof}

\begin{problem}{16}
    Suppose \(T \in \Lc(V)\). Define \(g \colon \F \to \F\) by \(g(x) = \det (I + xT)\). Show 
    that \(g'(0) = \tr \ T\). 
\end{problem}

\begin{proof}
\begin{align*}
    g'(x) &= \frac{d}{dx} \det (I + x T) \\ 
    &= \frac{d}{dx} \prod_{i=1}^{n} (1 + x \lambda_i) \\ 
    &= \sum_{i=1}^{n} \left( \lambda_i \prod_{j \neq i} \left( 1 + x \lambda_i \right) \right) \\ 
\end{align*}

Substitute \(x = 0\) yields that 

\[g'(0) = \sum_{i=1}^{n} \lambda_i = \tr \ T\]
\end{proof}

\begin{problem}{19}
    Suppose \(V\) is an inner product space, \(e_1, \ldots, e_n\) is an orthonormal basis of \(V\), 
    and \(T \in \Lc(V)\) is a positive operator. 
    \begin{enumerate}[label=(\alph*)]
        \item Prove that \(\det T \leq \prod_{k=1}^{n} \langle Te_k,e_k \rangle\). 
        \item Prove that if \(T\) is invertible, then the inequality in (a) is an equality if and only 
        if \(e_k\) is an eigenvector of \(T\) for each \(k = 1, \ldots, n\). 
    \end{enumerate}
\end{problem}

\begin{proof}
\begin{enumerate}[label=(\alph*)]
    \item The matrix representation of \(T\) wrt. \(e_1, \ldots, e_n\) is that 
    \(A_{ij} = \langle Te_i, e_j \rangle\). Hence the r.h.s of this inequality is simply 
    the product of all the diagonal terms on the matrix of \(T\). We prove this inequality 
    through Cholesky factorization. Note that 
    \begin{align*}
        A = L L^*
    \end{align*}
    for lower-triangular matrix \(L\), and thus we have 
    \[\det T = \det A = (\det L)^2 = \left( \prod_{k=1}^{n} l_{kk} \right)^2 = \prod_{k=1}^{n}L_{kk}^2\]

    We note that 
    \[A_{kk} = L_{kk}^2 + \sum_{j=1}^{k-1} L_{kj}^2  \]
    and thus we have 
    \[\det A \leq \prod_{k=1}^{n}A_{kk} = \prod_{k=1}^{n} \langle Te_k,e_k \rangle\]
    
    % By P11 in ch7, we know that \(\langle Te_i,e_i \rangle = s_i\) wher \(s_i\) is the \(i\)-th 
    % singular value. Then we have that 
    % \[\det T \leq \prod_{k=1}^{n} s_i = \prod_{k=1}^{n} \langle Te_k, e_k \rangle\]
    
    \item  If \(e_k\) is an eigenvector of \(T\), then \(\langle Te_k, e_k \rangle = \lambda_k\), the 
    \(k\)-th eigenvalue of \(T\). Then we know that \(\det T \) is the product of all eigenvalues. 

    Conversely, if (a) is an equality, then we know that \(L\) is a diagonal matrix and thus 
    \(A\) is also a diagonal matrix. Then the orthonormal basis \(e_1, \ldots, e_n\) actually 
    diagonaizes \(T\) and hence each of them is an eigenvector of \(T\).
\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9D %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\section*{9D: Tensor Products}
\addcontentsline{toc}{section}{9D: Tensor Products}

\begin{definition}[bilinear functional on \(V \times W\), the vector space \(\Bcal(V, W)\)]
    A \textbf{bilinear functional} on \(V \times W\) is a function  \(\beta \colon V \times W \to \F\)
    such that \(v \mapsto \beta(v, w)\) is a linear functional on \(V\) for each \(w \in W\) and 
    \(w \mapsto \beta(v, w)\) is a linear functional on \(W\) for each \(v \in V\). 

    The vector space of bilinear functionals on \(V \times W\) is denoted by \(\Bcal(V, W)\). 
\end{definition}

\begin{remark}
    If \(V = W\), then a bilinear functional on \(V \times W\) is a bilinear form.
\end{remark}

\begin{corollary}
    \(\dim \Bc(V, W) = (\dim V) ( \dim W)\)
\end{corollary}

\begin{remark}
    We want a \emph{basis-free} definition of the tensor product. 
\end{remark}

\begin{definition}[tensor product, \(V \otimes W, v \otimes w\)]
    The \textbf{tensor product} \(V \otimes W\) is defined to be \(\Bc(V', W')\). 

    For \(v \in V\) and \(w \in W\), the \textbf{tensor product} \(v \otimes w\) is the element 
    of \(V \otimes W\) defined by 
    \[(v \otimes w)(\varphi, \tau) = \varphi (v) \tau(w)\]
    for all \((\varphi, \tau) \in V' \times W'\). 
\end{definition}

\begin{corollary}
    \(\dim (V \otimes W) = (\dim V) (\dim W)\).
\end{corollary}

\begin{proposition}[bilinearity of tensor product]
    Suppose \(v, v_1, v_2 \in V\) and \(w, w_1, w_2 \in W\) and \(\lambda \in \F\). Then 
    \[(v_1 + v_2) \otimes w = v_1 \otimes w + v_2 \otimes w \text{   and   }
    v \otimes (w_1 +w_2) = v \otimes w_1 + v \otimes w_2\]
    and 
    \[\lambda \left( v \otimes w \right) = (\lambda v) \otimes w = v \otimes \left( \lambda w \right)\] 
\end{proposition}

\begin{thm}[basis of \(V \otimes W\)]
    Suppose \(e_1, \ldots, e_m\) is a list of vectors in \(V\) and \(f_1, \ldots, f_n\) is a list 
    of vectors in \(W\). 

    \begin{enumerate}[label=(\alph*)]
        \item If \(e_1, \ldots, e_m\) and \(f_1, \ldots, f_n\) are both linearly independent list, then 
        \[\{e_j \otimes f_k\}_{j= 1, \ldots, m ; k = 1, \ldots, n}\]
        is a linearly independent list in \(V \otimes W\).

        \item If \(e_1, \ldots, e_m\) is a basis of \(V\) and \(f_1, \ldots, f_n\) is a basis of \(W\), then 
        the list \(\{e_j \otimes f_k\}_{j=1, \ldots, m; k = 1, \ldots, n}\) is a basis of \(V \otimes W\). 
    \end{enumerate}
\end{thm}

\begin{definition}[bilinear map]
    A \textbf{bilinear map} from \(V \times W\) to a vector space \(U\) is a function 
    \(\Gamma \colon V \times W \to U\) such that \(v \mapsto \Gamma(v, w)\) is a linear map 
    from \(V\) to \(U\) for each \(w \in W\) and \(w \mapsto \Gamma(v, w)\) is a linear map 
    from \(W\) to \(U\) for each \(v \in V\).
\end{definition}

\begin{lemma}[converting bilinear maps to linear maps]
    Suppose \(U\) is a vector space. 

    \begin{enumerate}[label=(\alph*)]
        \item Suppose \(\Gamma \colon V \times W \to U\) is a bilinear map. Then there exists 
        a unique linear map \(\tilde{\Gamma} \colon V \otimes W \to U\) such that 
        \[\tilde{\Gamma} (v \otimes w) = \Gamma(v, w)\]
        for all \((v, w) \in V \times W\).
        
        \item Conversely, suppose \(T \colon V \otimes W \to U\) is a linear map. Then there 
        exists a unique bilinear map \(T^{\#} \colon V \times W \to U\) such that 
        \[T^{\#} (v, w) = T(v \otimes w)\]
        for all \((v, w) \in V \times W\).
    \end{enumerate}
\end{lemma}

\begin{thm}[inner product on tensor product of two inner product spaces]
    Suppose \(V\) and \(W\) are inner product spaces. Then there is a unique inner product 
    on \(V \otimes W\) such that 
    \[\langle v \otimes w, u \otimes x \rangle = \langle v,u \rangle \langle w,x \rangle\]
    for all \(u, v \in V\) and \(w, x \in W\).
\end{thm}

\begin{remark}
    We have that \(\norm{v \otimes w} = \norm{v} \norm{w}\).
\end{remark}

\begin{corollary}
    Suppose \(V\) and \(W\) are inner product spaces, and \(e_1, \ldots, e_m\) is an orthonormal 
    basis of \(V\) and \(f_1, \ldots, f_n\) is an orthonormal basis of \(W\). Then 
    \[ \{e_j \otimes f_k\}_{j = 1, \ldots, m;k=1, \ldots, n}\]
    is an orthonormal basis of \(V \otimes W\).
\end{corollary}

\begin{definition}
    An \textbf{m-linear} functional on \(V_1 \times \cdots \times V_m\) is a function 
    \(\beta \colon V_1 \times \cdots \times V_m \to \F\) that is a linear functional in 
    each slot when the other slots are held fixed. 

    The vector space of \(m\)-linear functionals on \(V_1 \times \cdots \times V_m\) is denoted by 
    \(\Bc(V_1, \ldots, V_m)\).
\end{definition}

\begin{corollary}
    \(\dim \Bcal (V_1, \ldots, V_m) = (\dim V_1) \times \cdots \times (\dim V_m)\)
\end{corollary}

\begin{definition}[tensor product]
    The tensor product \(V_1 \otimes \cdots \otimes V_m\) is defined to be \(\Bc(V_1', \ldots, V_m')\). 

    For \(v_1 \in V_1, \ldots, v_m \in V_m\), the \textbf{tensor product} \(v_1 \otimes \cdots \otimes v_m\)
    is the element of \(V_1 \otimes \cdots \otimes V_m\) defined by 
    \[(v_1 \otimes \cdots \otimes v_m) (\varphi_1, \ldots, \varphi_m) = \varphi_1(v_1)\cdots \varphi_m(v_m)\]
    for all \((\varphi_1, \ldots, \varphi_m) \in V_1' \times \cdots \times V_m'\).
\end{definition}

\begin{corollary}
    Suppose \(\dim V_k = n_k\) and \(e_1^k, \ldots, e_{n_k}^k\) is a basis of \(V_k\) for 
    \(k = 1, \ldots, m\). Then 
    \[\{e_{j_1}^1 \otimes \cdots \otimes e_{j_m}^m\}_{j_1 = 1, \ldots, n_1; \ldots; j_m = 1, \ldots, n_m}\]
    is a basis of \(V_1 \otimes \cdots \otimes V_m\).
\end{corollary}

\begin{definition}[m-linear map]
    An \(m\)-linear map from \(V_1 \times \cdots \times V_m\) to a vector space \(U\) is a function 
    \(\Gamma \colon V_1 \times \cdots \times V_m \to U\) that is a linear map in each slot when the other 
    slots are held fixed. 
\end{definition}

\begin{thm}[converting m-linear map to linear maps]
    Suppose \(U\) is a vector space. 

    \begin{enumerate}[label=(\alph*)]
        \item Suppose that \(\Gamma \colon V_1 \times \cdots \times V_m \to U\) is an \(m\)-linear map. 
        Then there exists a unique linear map \(\tilde{\Gamma} \colon V_1 \otimes \cdots \otimes V_m \to U\) 
        such that 
        \[\tilde{\Gamma}(v_1 \otimes \cdots \otimes v_m) = \Gamma(v_1, \ldots, v_m)\]
        for all \((v_1, \ldots, v_m) \in V_1 \times \cdots \times V_m\).
        
        \item Conversely, suppose \(T \colon V_1 \otimes \cdots \otimes V_m \to U\) is a linear map. 
        Then there exists a unique \(m\)-linear map \(T^{\#} \colon V_1 \times \cdots \times V_m \to U\)
        such that 
        \[T^\# (v_1, \ldots, v_m) = T(v_1 \otimes \cdots \otimes v_m)\]
        for all \((v_1, \ldots, v_m) \in V_1 \times \cdots \times V_m\). 
    \end{enumerate}
\end{thm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 9D PS %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage 
\addcontentsline{toc}{subsection}{9D Problem Sets}

\begin{problem}{1}
    Suppose \(v \in V\) and \(w \in W\). Prove that \(v \otimes w = 0\) if and only if 
    \(v = 0\) or \(w = 0\).
\end{problem}

\begin{proof}
By definition, we have for any \((\varphi, \tau) \in V' \times W'\), 
\[(v \otimes w) (\varphi, \tau) = \varphi (v) \tau(w)\]

Then this means that \(\varphi(v) \tau(w) = 0\) for arbitrary choice of \(\varphi, \tau\), meaning 
that either \(v = 0\) or \(w = 0\).
\end{proof}

\begin{problem}{3}
    Suppose that \(v_1, \ldots, v_m\) is a linearly independent list in \(V\). Suppose also 
    that \(w_1, \ldots, w_m\) is a list in \(W\) such that 
    \[v_1 \otimes w_1 + \cdots + v_m \otimes w_m = 0\]
    Prove that \(w_1 = \cdots = w_m = 0\).
\end{problem}

\begin{proof}
By the linear map lemma and the linear independence of \(v_1, \ldots, v_m\), there exists 
\(\varphi_1, \ldots, \varphi_m \in V'\) such that 
\[ \varphi_j(v_k) = \begin{cases}
    1 &\text{if } j = k \\ 
    0 &\text{if } j \neq k 
\end{cases}\]

where \(j, k \in \{1, \ldots, m\}\). Applying such \(\{\varphi_i\}_{i=1}^m\) to the list 
\[ \sum_{i=1}^{n} v_i \otimes w_i\]
and take \(\tau \in W'\) to be the identity map yields that 
\[w_1 = \cdots = w_m =  0\] 
\end{proof}

\begin{problem}{5}
    Suppose \(m\) and \(n\) are positive integers. For \(v \in \F^m\) and \(w \in \F^n\), 
    identify \(v \otimes w\) with an \(m\)-by-\(n\) matrix as in Example 9.76. With that identification, 
    show that the set 
    \[\{v \otimes w \colon v \in \F^m \text{ and } w \in \F^n\}\]
    is the set of \(m\)-by-\(n\) matrix matrices (with entries in \(\F\)) that have rank at most one. 
\end{problem}

\begin{proof}
If one examine the matrices with entries shown on the matrix, it's easy to tell that 
for row \(j\) and row \(k\) with \(j \neq k\), one can get row \(k\) from row \(j\) 
through multiplying \(v_k / v_j\). The same applies to arbitrary pairs of columns. Thus 
the matrix has at most rank one. 
\end{proof}

\begin{problem}{8}
    Suppose \(v_1, \ldots, v_m \in V\) and \(w_1, \ldots, w_m \in W\) are such that 
    \[ v_1 \otimes w_1 + \cdots + v_m \otimes w_m = 0\]
    Suppose that \(U\) is a vector space and \(\Gamma \colon V \times W \to U\) is a bilinear 
    map. Show that 
    \[ \Gamma(v_1, w_1) + \cdots + \Gamma(v_m, w_m) = 0\]
\end{problem}

\begin{proof}
We know there exists a unique ``converting'' linear map \(\tilde{\Gamma}\) such that 
\[\tilde{\Gamma(v \otimes w)} = \Gamma(v, w)\]
Hence, applying this gives that 
\begin{align*}
    \sum_{i=1}^{m} \Gamma (v_i, w_i) 
    &= \sum_{i=1}^{m} \tilde{\Gamma}(v_i \otimes w_i) \\ 
    &= \tilde{\Gamma} \left( \sum_{i=1}^{m} v_i \otimes w_i \right) \\ 
    &= 0 
\end{align*}
\end{proof}

\end{document}